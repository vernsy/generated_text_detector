# Evaluation
Three things can happen here: Calculating the metrics, getting the calibration of a model, and retrieving the tokens of a text if it would have been generated by LLaMA.

## metrics
The functions used for evaluation in the `model` folder are defined here. Also it logs to here. Nothing has to be executed.

## calibration
The functions used to plot the calibration and to calculate the ECE value are stored here. Nothing has to be executed.

## reverse llama token probability
The script uses the text-dataset for error analysis, that was created before, with the true positives, true negatives, ... stored in `config.FILENAME_MIXED_TEXTS_TRUE_AND_FALSE_POSITIVES_NEGATIVES`

```
torchrun --nproc_per_node 2 03_logprobs_error_analysis.py --ckpt_dir /data/LLaMA/13B/ --tokenizer_path /data/LLaMA/tokenizer.model --out_path error_analysis_xyz --logging_path xyz.log
```
the probabilities are outputted to `config.FILENAME_PROBABILITIES_ERROR_ANALYSIS`.
Since LLaMA does everything double, again (bc. of the batch size 1), execute the unix-script `delete_duplicates.sh` by first replacing the filenames inside with the correct names.